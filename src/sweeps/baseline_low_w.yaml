program: DRL-Traj-Planner/src/continous_training_sweep.py
method: grid
metric:
  name: eval/episode_success
  goal: maximize
parameters:
  algo:
    value: "sac"
  map_key:
    value: "random"
  reward_mode:
    value: "sum"
  env_name:
    value: "TrajectoryPlannerEnvironmentImgsReward3-v0"
  seed:
    values: [1235, 2621, 12, 52]
  w1:
    value: 0.05  # speed
  w2:
    value: 0.05  # acceleration
  w4:
    value: 0.05  # cross track TODO how do I make sure to not change the true reward?
  sac:
    parameters:
      total_frames:
        value: 1000000
      replay_buffer_size:
        value: 800000
      curriculum:
        parameters:
          base_reward_keys:
            value: "gdcsax"
          all_reward_keys:
            value: "gdcsax"
